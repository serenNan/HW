# HW2: Windy Gridworld 强化学习作业

## 📋 作业概述

本作业基于经典的 **Windy Gridworld（风力网格世界）** 问题，要求实现和比较 SARSA 和 Q-learning 算法，并测试 LLM 在强化学习任务中的表现。

---

## 🌍 环境设定

### 基本设定

我们考虑具有以下变化的 Windy Gridworld：

#### 1. 网格结构
- **网格大小**：7行 × 10列
- **起始位置**：多个（见任务要求）
- **目标状态**：(4, 8) - 灰色方格
- **坐标系统**：(行, 列)，左下角为 (7, 1)，右上角为 (1, 10)

#### 2. 动作空间：8向移动（King's Moves）

允许对角线移动，因此每个方格有 **8 个可能的动作**（类似国际象棋中国王的移动方式）。

**动作集合**：
```python
actions = {'n', 'ne', 'e', 'se', 's', 'sw', 'w', 'nw'}
# 分别代表：北、东北、东、东南、南、西南、西、西北
```

**初始策略**：每个方向的概率均为 **1/8**（均匀随机策略）

**动作效果**（无风情况）：
| 动作 | 坐标变化 | 说明 |
|------|---------|------|
| n    | (-1, 0) | 向上移动1格 |
| ne   | (-1, 1) | 向右上移动 |
| e    | (0, 1)  | 向右移动1格 |
| se   | (1, 1)  | 向右下移动 |
| s    | (1, 0)  | 向下移动1格 |
| sw   | (1, -1) | 向左下移动 |
| w    | (0, -1) | 向左移动1格 |
| nw   | (-1, -1)| 向左上移动 |

#### 3. 特殊状态

##### 🎯 目标状态（Goal State）
- **位置**：(4, 8)
- **奖励**：+10（推测，需要从原始代码确认）
- **终止**：是（episode结束）
- **颜色标记**：灰色/绿色方格

##### ☠️ 死亡状态（Death State）
- **位置**：(1, 10)
- **奖励**：-100
- **终止**：是（episode结束）
- **颜色标记**：黑色方格
- **说明**：这是一个需要避免的危险状态

##### 🔵 吸收状态（Absorbing State）- 仅任务7
- **位置**：(1, 4)
- **奖励**：+5
- **终止**：是（episode结束）
- **颜色标记**：蓝色方格
- **说明**：任务7中新增的状态

#### 4. 风力效果（Wind Effect）

特定列存在向上的风，会影响 agent 的垂直移动：

| 列编号 | 风力强度 | 效果 |
|--------|---------|------|
| 4, 5, 6 | 1 | 向上推1格（行号-1） |
| 7, 8   | 2 | 向上推2格（行号-2） |
| 9      | 1 | 向上推1格（行号-1） |
| 其他列 | 0 | 无风 |

**风力计算规则**：
- 风只影响**垂直方向**的移动
- 在执行动作后，根据**目标列**计算风的影响
- 风向始终向上（减小行号）

**示例1**：
```
当前位置: (6, 4)
执行动作: 'ne' (东北)
无风结果: (5, 5)
实际结果: (4, 5)  ← 列5有1格风，向上推1格
```

**示例2**：
```
当前位置: (6, 7)
执行动作: 'se' (东南)
无风结果: (7, 8)
实际结果: (5, 8)  ← 列8有2格风，向上推2格
```

**示例3**：
```
当前位置: (6, 7)
执行动作: 'e' (东)
无风结果: (6, 8)
实际结果: (4, 8)  ← 列8有2格风，向上推2格，正好到达目标！
```

#### 5. 奖励结构

| 情况 | 奖励 | 终止 |
|------|------|------|
| 到达目标 (4,8) | +10 | 是 |
| 到达死亡 (1,10) | -100 | 是 |
| 到达吸收 (1,4) | +5 | 是（仅任务7） |
| 普通移动 | -1 | 否 |

**设计思想**：
- 每步 -1 鼓励寻找最短路径
- 目标 +10 是最优选择
- 死亡 -100 强烈惩罚，必须避免
- 吸收 +5 次优选择（任务7）

---

## 📝 任务要求

### 任务 1：基础算法实现（8向移动）

**要求**：使用 **SARSA** 和 **Q-learning** 找到最优策略

**起始位置**（3个）：
- (7, 1) - 左下角
- (4, 1) - 左侧中间
- (1, 7) - 上方

**需要输出**：
- 每个起点的最优策略
- 最优路径（状态序列）
- 路径长度（步数）
- 学习曲线（每个episode的步数）
- 策略可视化（箭头图）

**对比内容**：
- SARSA vs Q-learning 的收敛速度
- 最优路径是否一致
- Q值的差异

---

### 任务 2：Rook移动对比（4向移动）

**要求**：如果**不允许对角线移动**（只允许 Rook 移动），会发生什么？

**Rook 移动**（4向移动）：
```python
actions = {'n', 'e', 's', 'w'}  # 只有上、右、下、左
```

**对比分析**：
1. 路径长度的差异
2. 是否更容易触碰死亡状态
3. 收敛速度的变化
4. 最优策略的复杂度

**预期差异**：
- 4向移动路径更长
- 可能更难避开死亡状态
- 探索空间减小，但可能更难收敛

---

### 任务 3：LLM 自动算法选择

**要求**：让 LLM 为这个问题生成代码，**不指定使用哪个算法**，观察它选择什么

**测试方法**：
```
提示词示例：
"请为 Windy Gridworld 问题编写强化学习代码，
环境设定如下：[描述环境]，找到从 (7,1) 到 (4,8) 的最优策略。"
```

**观察内容**：
- LLM 选择了什么算法？（SARSA / Q-learning / 其他）
- 代码质量如何？
- 是否正确实现了环境？
- 能否找到最优策略？

---

### 任务 4：LLM 算法对比验证

**要求**：
1. 让 LLM **分别生成** SARSA 和 Q-learning 的代码
2. 运行这些代码
3. **检查结果是否与你自己的代码相同**

**验证内容**：
- 路径长度是否一致
- 最优策略是否相同
- 学习曲线是否相似
- 是否存在实现错误

**可能的发现**：
- LLM 可能使用不同的参数
- 代码风格差异
- 某些边界情况的处理不同

---

### 任务 5：纯推理最优轨迹

**要求**：**仅用模型描述**让 LLM 生成最优轨迹，**不编写代码**

**测试方法**：
```
提示词示例：
"在 Windy Gridworld 环境中（7×10网格，风力效果如下...），
如果从 (1,1) 开始并遵循最优策略，请直接给出到达 (4,8) 的轨迹。"
```

**评估标准**：
- 是否正确考虑了风的影响
- 是否避开了死亡状态
- 路径是否合理
- 是否真的是最优路径

**这个任务测试**：LLM 的推理能力和对物理规则的理解

---

### 任务 6：LLM 的 4向移动表现

**要求**：看看如果**不允许对角线移动**，LLM 会怎么做

**测试场景**：
- 任务3：让 LLM 自动选择算法（4向）
- 任务4：验证 SARSA 和 Q-learning（4向）
- 任务5：纯推理最优轨迹（4向）

**观察重点**：
- LLM 是否能适应约束条件
- 4向移动下的策略质量
- 与8向移动结果的对比

---

### 任务 7：吸收状态变种

**要求**：在位置 **(1, 4)** 添加一个**奖励为 +5** 的吸收状态，对这个新网格**重复任务 1, 2, 3, 4, 5, 6**

#### 新环境设定
```
目标状态 (4,8)：奖励 +10
吸收状态 (1,4)：奖励 +5  ← 新增
死亡状态 (1,10)：奖励 -100
普通移动：奖励 -1
```

#### 策略分析

**奖励权衡**：
- 直接到目标：~10步 × (-1) + 10 = 0
- 绕路到吸收：~7步 × (-1) + 5 = -2
- 到达死亡：~10步 × (-1) + (-100) = -110

**可能的策略变化**：
1. 大部分起点仍选择目标（奖励更高）
2. 某些起点可能选择吸收状态（如果更近）
3. 需要平衡路径长度与奖励大小

#### 重复所有任务

对于吸收状态环境，完成：
- ✓ 任务1：SARSA 和 Q-learning（8向）
- ✓ 任务2：4向移动对比
- ✓ 任务3：LLM 自动选择
- ✓ 任务4：验证一致性
- ✓ 任务5：纯推理轨迹
- ✓ 任务6：4向移动下的 LLM 表现

---

## 🎨 网格世界可视化

### 网格布局（7行×10列）

```
行/列  1   2   3   4   5   6   7   8   9  10
  1   [ ] [ ] [ ] [🔵] [ ] [ ] [ ] [ ] [ ] [☠️]
  2   [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]
  3   [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]
  4   [ ] [ ] [ ] [ ] [ ] [ ] [ ] [🎯] [ ] [ ]
  5   [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]
  6   [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]
  7   [S1][ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]

风力： 0   0   0   1   1   1   2   2   1   0
      ↑   ↑   ↑   ↑   ↑   ↑   ↑↑  ↑↑  ↑   ↑
```

**图例**：
- 🎯 (4,8)：目标状态（绿色方格）
- ☠️ (1,10)：死亡状态（黑色方格）
- 🔵 (1,4)：吸收状态（蓝色方格，仅任务7）
- S1 (7,1)：起始位置示例
- [ ]：普通方格
- ↑/↑↑：风力方向和强度

---

## 🔧 实现提示

### SARSA 算法（On-Policy）

```python
def sarsa(env, start_state, episodes, alpha=0.1, gamma=1.0, epsilon=0.1):
    """
    SARSA: State-Action-Reward-State-Action
    On-policy TD control
    """
    Q = initialize_q_table()

    for episode in range(episodes):
        state = start_state
        action = epsilon_greedy_policy(Q, state, epsilon)

        while not is_terminal(state):
            next_state, reward = env.step(state, action)
            next_action = epsilon_greedy_policy(Q, next_state, epsilon)

            # SARSA update
            Q[state, action] += alpha * (
                reward + gamma * Q[next_state, next_action] - Q[state, action]
            )

            state = next_state
            action = next_action  # 使用实际选择的动作

    return Q
```

### Q-Learning 算法（Off-Policy）

```python
def q_learning(env, start_state, episodes, alpha=0.1, gamma=1.0, epsilon=0.1):
    """
    Q-Learning: Off-policy TD control
    """
    Q = initialize_q_table()

    for episode in range(episodes):
        state = start_state

        while not is_terminal(state):
            action = epsilon_greedy_policy(Q, state, epsilon)
            next_state, reward = env.step(state, action)

            # Q-Learning update
            Q[state, action] += alpha * (
                reward + gamma * max(Q[next_state, :]) - Q[state, action]
            )

            state = next_state  # 使用max而不是实际动作

    return Q
```

### 关键参数建议

| 参数 | 建议值 | 说明 |
|------|--------|------|
| alpha (学习率) | 0.1 | 控制更新幅度 |
| gamma (折扣因子) | 1.0 | 无折扣（episodic任务） |
| epsilon (探索率) | 0.1 - 0.3 | 10%-30%探索 |
| episodes | 10000 - 50000 | 训练轮数 |

---

## 📊 评估标准

### 1. 算法性能
- ✓ 能否找到最优路径
- ✓ 收敛速度
- ✓ 学习曲线的平滑度

### 2. 路径质量
- ✓ 是否避开死亡状态
- ✓ 路径长度是否最优
- ✓ 是否正确利用风力

### 3. 对比分析
- ✓ SARSA vs Q-learning 的差异
- ✓ 8向 vs 4向的差异
- ✓ 有无吸收状态的策略变化

### 4. LLM 评估
- ✓ 代码质量
- ✓ 算法选择的合理性
- ✓ 推理能力
- ✓ 结果准确性

---

## 🔑 关键术语对照表

| 中文 | 英文 | 缩写 |
|------|------|------|
| 强化学习 | Reinforcement Learning | RL |
| 状态 | State | S |
| 动作 | Action | A |
| 奖励 | Reward | R |
| 策略 | Policy | π |
| 价值函数 | Value Function | V |
| Q值函数 | Q-Value Function | Q |
| 时序差分 | Temporal Difference | TD |
| on-policy | 同策略 | - |
| off-policy | 异策略 | - |
| epsilon-greedy | ε-贪心策略 | - |
| episode | 回合/轨迹 | - |

---

## ⚠️ 注意事项

### 1. 坐标系统
- 注意行号的方向（向上减小）
- 风的效果是减小行号（向上推）
- 确保边界检查正确

### 2. 终止状态
- 目标、死亡、吸收状态都是终止状态
- 到达终止状态后episode结束
- 不要在终止状态继续更新Q值

### 3. 风力计算
- 风只在目标列有效
- 先执行动作，再应用风
- 边界检查在风之后

### 4. 奖励设计
- 确认目标状态的奖励值（推测+10）
- 死亡状态必须是-100
- 每步-1鼓励最短路径

### 5. 参数调优
- epsilon太小：探索不足，可能找不到目标
- epsilon太大：收敛慢
- episodes太少：可能未收敛
- alpha影响学习速度和稳定性

---

## 📖 参考资料

1. **Sutton & Barto** - Reinforcement Learning: An Introduction (2nd Edition)
   - Chapter 6.4: Sarsa: On-Policy TD Control
   - Chapter 6.5: Q-learning: Off-Policy TD Control
   - Example 6.5: Windy Gridworld

2. **原始 Windy Gridworld**
   - 本作业是经典问题的变种
   - 主要变化：8向移动、死亡状态、吸收状态

---

## ✅ 提交清单

- [ ] 任务1：SARSA和Q-learning实现（8向）
- [ ] 任务2：4向移动对比分析
- [ ] 任务3：LLM自动选择算法的测试
- [ ] 任务4：LLM代码验证
- [ ] 任务5：LLM纯推理测试
- [ ] 任务6：LLM的4向移动表现
- [ ] 任务7：吸收状态环境下的完整测试
- [ ] 学习曲线图
- [ ] 策略可视化图
- [ ] 分析报告

---

**Good Luck! 🚀**
