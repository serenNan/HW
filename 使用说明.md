# HW2 作业使用说明

## 📁 文件说明

- `HW2_Solution.ipynb` - **完整实现**（包含任务1、2、7的代码）
- `SARSA.ipynb` - 原始文件（保留）
- `HW2.pdf` - 作业说明
- `CLAUDE.md` - 项目说明文档

## 🚀 快速开始0

### 1. 激活环境

```bash
conda activate rl-hw
```

### 2. 启动 Jupyter

```bash
# 方式1: Jupyter Notebook（经典界面）
jupyter notebook

# 方式2: JupyterLab（推荐，现代界面）
jupyter lab
```

### 3. 打开文件

在浏览器中打开 `HW2_Solution.ipynb`

### 4. 运行代码

**方式A - 全部运行**：
- 点击菜单：`Cell` → `Run All`
- 或快捷键：`Shift + Enter` 逐个运行

**方式B - 分步运行**：
- 依次运行每个cell（`Shift + Enter`）
- 可以查看每一步的输出

## 📊 实现内容

### ✅ 已实现的任务

#### **任务1**: SARSA和Q-learning在三个起点的最优策略（8向移动）
- 起点：(7,1), (4,1), (1,7)
- 算法：SARSA、Q-Learning
- 输出：最优路径、路径长度、可视化

#### **任务2**: 仅允许Rook移动（4向）的对比
- 对比8向 vs 4向移动的差异
- 路径长度对比
- 策略可视化

#### **任务7**: 添加吸收状态(1,4)，奖励+5
- 8向移动 + 吸收状态
- 4向移动 + 吸收状态
- 分析吸收状态对策略的影响

### 📝 需要你手动完成的任务（任务3-6）

这些任务需要与LLM交互测试，**无需编程**：

#### **任务3**: 测试LLM代码生成（不指定算法）
**提示词示例**：
```
请为Windy Gridworld问题生成代码，要求：
- 7x10网格
- 8向移动
- 死亡状态(1,10)奖励-100
- 目标状态(4,8)
- 从(7,1)出发找最优策略

不要指定使用哪个算法，让LLM自己选择。
```

**观察**：LLM选择了哪个算法？（SARSA / Q-Learning / 其他）

#### **任务4**: 测试LLM分别生成SARSA和Q-learning代码
**提示词A**：
```
使用SARSA算法解决上述Windy Gridworld问题
```

**提示词B**：
```
使用Q-Learning算法解决上述Windy Gridworld问题
```

**验证**：对比两个算法的结果是否一致

#### **任务5**: 测试LLM仅通过描述生成最优轨迹
**提示词示例**：
```
不要编写代码，只通过推理：
在Windy Gridworld环境中（环境描述...），
从(1,1)出发，遵循最优策略会经过哪些状态？
请列出完整的状态序列。
```

**验证**：与你的实现对比，轨迹是否正确

#### **任务6**: 测试不允许对角线移动的情况
**提示词示例**：
```
重复任务5，但只允许4向移动（上下左右），不允许对角线移动
```

## 🎯 核心代码说明

### 环境类 `WindyGridworld`

```python
# 创建8向移动环境
env = WindyGridworld(king_moves=True)

# 创建4向移动环境
env = WindyGridworld(king_moves=False)

# 创建带吸收状态的环境
env = WindyGridworld(king_moves=True, absorbing_state=(1,4))
```

### 算法调用

```python
# SARSA
Q, steps = sarsa(env, start_state=(7,1), episodes=5000)

# Q-Learning
Q, steps = q_learning(env, start_state=(7,1), episodes=5000)
```

### 提取策略和路径

```python
# 从Q表提取最优策略
policy = get_optimal_policy(Q, env)

# 生成最优路径
path = get_optimal_path(policy, env, start_state=(7,1))
```

## 📈 预期结果

### 任务1结果（8向移动）
- **(7,1) 起点**：预计路径长度 ~7-9步
- **(4,1) 起点**：预计路径长度 ~7-9步
- **(1,7) 起点**：预计路径长度 ~3-5步

### 任务2结果（4向移动）
- 路径长度比8向移动**更长**
- 策略更依赖于正确处理风的影响

### 任务7结果（吸收状态）
- 某些起点可能改变策略
- 需要权衡：绕路获得+5 vs 直接到目标

## 🔧 调试提示

### 如果路径过长或不收敛

1. **增加训练回合数**：
```python
Q, steps = sarsa(env, start, episodes=10000)  # 从5000增加到10000
```

2. **调整学习率**：
```python
Q, steps = sarsa(env, start, alpha=0.2)  # 尝试0.05-0.5
```

3. **调整探索率**：
```python
Q, steps = sarsa(env, start, epsilon=0.05)  # 降低探索
```

### 如果程序运行很慢

- 减少训练回合数（如1000回合用于快速测试）
- 减少可视化次数

### 如果可视化不显示

- 确保matplotlib正确安装
- 在Jupyter中使用 `%matplotlib inline`

## 📚 参考资料

- Sutton & Barto《Reinforcement Learning: An Introduction》第6章
- 标准Windy Gridworld问题定义
- SARSA vs Q-Learning对比

## ✨ 完成检查清单

- [ ] 任务1：SARSA和Q-Learning结果（3个起点）
- [ ] 任务2：4向移动对比
- [ ] 任务3：LLM自动选择算法测试
- [ ] 任务4：LLM生成SARSA和Q-Learning对比
- [ ] 任务5：LLM推理最优轨迹（8向）
- [ ] 任务6：LLM推理最优轨迹（4向）
- [ ] 任务7：吸收状态实验

## 💡 作业提交建议

1. **运行 `HW2_Solution.ipynb`**，保存所有输出
2. **导出PDF**：`File` → `Download as` → `PDF`
3. **截图关键结果**：
   - 三个起点的最优路径可视化
   - 学习曲线
   - 路径长度对比表
4. **记录LLM测试结果**（任务3-6）
5. **撰写分析报告**：
   - SARSA vs Q-Learning的异同
   - 8向 vs 4向移动的影响
   - 吸收状态的策略变化
   - LLM的表现评估

## ❓ 常见问题

**Q: 参数设置合理吗？**
A: 代码使用标准Windy Gridworld设置，如果与老师要求不同，可以修改环境类中的参数。

**Q: 如何修改目标状态位置？**
A: 在 `WindyGridworld` 类的 `__init__` 方法中修改 `self.goal_state = (4, 8)`

**Q: 风向设置正确吗？**
A: 基于PDF描述，列4,5,6,9风力1，列7,8风力2。如需调整，修改 `self.wind` 字典。

**Q: 如何增加新的起点测试？**
A: 在 `start_states` 列表中添加坐标即可。

## 🎓 学习建议

1. **理解算法差异**：仔细对比SARSA和Q-Learning的更新公式
2. **观察收敛过程**：查看学习曲线，理解探索-利用权衡
3. **分析策略**：观察可视化中的箭头，理解agent如何应对风
4. **实验参数**：尝试不同的α、ε、γ值，观察影响

祝你顺利完成作业！🎉
